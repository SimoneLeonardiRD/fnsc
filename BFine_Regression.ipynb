{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning BERT for Regression Tasks\n",
    "\n",
    "1. [Fine-tune BERT and for regression problem](https://discuss.huggingface.co/t/fine-tune-bert-and-camembert-for-regression-problem/332)\n",
    "2. [Modify BertForSequenceClassification](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1227)\n",
    "3. [Understand fine tuning](https://medium.com/@prakashakshay90/fine-tuning-bert-model-using-pytorch-f34148d58a37) phase with myPersonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup, BertModel\n",
    "from tqdm import tqdm, trange, tnrange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device, later in training loop we will load data into device\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "#n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)\n",
    "\n",
    "SEED = 19\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == torch.device(\"cuda\"):\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(device, n_gpu, torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read myPersonality dataset status and big5 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence\n",
      "8333  Guess whose place of work got broken into last...\n",
      "4013  You're going to want to read my latest article...\n",
      "5759                                good day today. =]]\n",
      "1363  is sad thinking about how her kitties are at h...\n",
      "7930  HAHAHA ~ love this!: \"\"\"\"Please put this on yo...\n",
      "9913\n"
     ]
    }
   ],
   "source": [
    "# Reading Data into dataFrame\n",
    "text = pd.read_csv(\"~/Venv/Documents/dataset/myPersonalitySmall/statuses_unicode.txt\", header=None, names=['sentence'])\n",
    "big5 = pd.read_csv(\"../dataset/myPersonalitySmall/big5labels.txt\", delimiter=\" \", header=None, names=['O', 'C', 'E', 'A', 'N'])\n",
    "print(text.sample(5))\n",
    "print(text.sentence.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence     O     C     E  \\\n",
      "666   seriously cannot figure out what the HELL is w...  4.15  3.10  3.35   \n",
      "8236  is completely disappointed with the *PROPNAME*...  4.25  3.15  3.20   \n",
      "1920                               has contacts finally  4.25  2.80  2.45   \n",
      "6837  can't decide if it's good or bad that med stud...  4.13  3.50  4.13   \n",
      "8835  Congratulations *PROPNAME*!!! Newest Federal A...  4.50  3.50  2.25   \n",
      "\n",
      "         A     N  \n",
      "666   2.85  2.75  \n",
      "8236  3.85  1.85  \n",
      "1920  3.80  4.00  \n",
      "6837  4.50  1.43  \n",
      "8835  3.25  2.75  \n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([text, big5], axis=1, sort=False)\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the max_seq_len in the dataset, replacing the default 512 of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in df['sentence']:\n",
    "    #print(len(str(i)))\n",
    "    if (len(str(i))>max_len):\n",
    "        max_len = len(str(i))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512 #the nearest power of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code in a virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets\n",
    "# jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the sentences according to BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence']= df['sentence'].astype('str') #from object to string (sometimes str sometimes string)\n",
    "sentences = df.sentence.values\n",
    "#print(sentences)\n",
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Actual sentence before tokenization: \",sentences[2])\n",
    "#print(\"Encoded Input from dataset: \",input_ids[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create attention mask\n",
    "attention_masks = []\n",
    "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "#print(attention_masks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.O.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a training set and a test set using a stratified k fold\n",
    "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=SEED,test_size=0.1)\n",
    "train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=SEED,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all our data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model changing the task from classification to regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1, output_hidden_states = True).to(device) # output_hidden_states ci permette di estrarre gli embeddings\n",
    "# setting num_label=1 configure the model to perform regression and change the loss into Mean-Square Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the change for the regression task is described at [transformers repository](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1227) on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "#Prepare optimizer and schedule (linear warmup and decay)\n",
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#     ]\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "#optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-b367fd0f34ff>:9: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1,epochs+1,desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a41161b7544200b031647afa5ffecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n"
     ]
    }
   ],
   "source": [
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "    # Calculate total loss for this epoch\n",
    "    batch_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "    \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
    "    \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate schedule\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear the previous accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Update tracking variables\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "    #store the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "        learning_rate.append(param_group['lr'])\n",
    "    \n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "    \n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "        hidden_states = outputs.hidden_states\n",
    "        layer_i = 13\n",
    "        for elem in hidden_states[layer_i]:\n",
    "            print(\"CLS batch\", elem[0])\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits[0].to('cpu').numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        tmp_eval_accuracy = accuracy_score(pred_flat, labels_flat)\n",
    "        tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "    print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}